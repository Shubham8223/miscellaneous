{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "import shutil"
      ],
      "metadata": {
        "id": "YUTKFsse5U8h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_filename = \"/content/archive (3).zip\"\n",
        "\n",
        "extract_dir = \"/content/\"\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "shutil.unpack_archive(zip_filename, extract_dir)\n"
      ],
      "metadata": {
        "id": "igapfbB4_7HP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story_path = \"/content/sherlock/sherlock\"\n",
        "\n",
        "def read_all_stories(story_path):\n",
        "    txt = []\n",
        "    for _, _, files in os.walk(story_path):\n",
        "        for file in files:\n",
        "            with open(os.path.join(story_path,file)) as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if line=='----------':\n",
        "                      break\n",
        "                    if line!='':\n",
        "                      txt.append(line)\n",
        "    return txt\n",
        "\n",
        "stories = read_all_stories(story_path)\n",
        "print(\"number of lines = \", len(stories))"
      ],
      "metadata": {
        "id": "Gh0eRccq5YkO",
        "outputId": "8817a86a-c267-4585-fa5d-5f46eaf8648f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of lines =  215021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "def clean_txt(txt):\n",
        "    cleaned_txt = []\n",
        "    for line in txt:\n",
        "        line = line.lower()\n",
        "        line = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-\\\\]\", \"\", line)\n",
        "        tokens = word_tokenize(line)\n",
        "        words = [word for word in tokens if word.isalpha()]\n",
        "        cleaned_txt+=words\n",
        "    return cleaned_txt\n",
        "\n",
        "cleaned_stories = clean_txt(stories)\n",
        "print(\"number of words = \", len(cleaned_stories))"
      ],
      "metadata": {
        "id": "idEiZDIy-7yP",
        "outputId": "adcf24f7-0417-4d03-e1c9-9f52d7d1e684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words =  2332247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_markov_model(cleaned_stories, n_gram=2):\n",
        "    markov_model = {}\n",
        "    for i in range(len(cleaned_stories)-n_gram-1):\n",
        "        curr_state, next_state = \"\", \"\"\n",
        "        for j in range(n_gram):\n",
        "            curr_state += cleaned_stories[i+j] + \" \"\n",
        "            next_state += cleaned_stories[i+j+n_gram] + \" \"\n",
        "        curr_state = curr_state[:-1]\n",
        "        next_state = next_state[:-1]\n",
        "        if curr_state not in markov_model:\n",
        "            markov_model[curr_state] = {}\n",
        "            markov_model[curr_state][next_state] = 1\n",
        "        else:\n",
        "            if next_state in markov_model[curr_state]:\n",
        "                markov_model[curr_state][next_state] += 1\n",
        "            else:\n",
        "                markov_model[curr_state][next_state] = 1\n",
        "\n",
        "    # calculating transition probabilities\n",
        "    for curr_state, transition in markov_model.items():\n",
        "        total = sum(transition.values())\n",
        "        for state, count in transition.items():\n",
        "            markov_model[curr_state][state] = count/total\n",
        "\n",
        "    return markov_model"
      ],
      "metadata": {
        "id": "AIGw8wfs_gYI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markov_model = make_markov_model(cleaned_stories)\n",
        "print(\"number of states = \", len(markov_model.keys()))\n",
        "print(\"All possible transitions from 'the game' state: \\n\")\n",
        "print(markov_model['the game'])"
      ],
      "metadata": {
        "id": "X8KKdf3q_hKx",
        "outputId": "893b1054-7069-47be-ef25-926b3367a7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of states =  208716\n",
            "All possible transitions from 'the game' state: \n",
            "\n",
            "{'was whist': 0.036036036036036036, 'is afoot': 0.036036036036036036, 'worth it': 0.02702702702702703, 'you are': 0.02702702702702703, 'was in': 0.02702702702702703, 'is hardly': 0.02702702702702703, 'would have': 0.036036036036036036, 'is up': 0.06306306306306306, 'is and': 0.036036036036036036, 'in their': 0.036036036036036036, 'was up': 0.09009009009009009, 'in that': 0.036036036036036036, 'the lack': 0.036036036036036036, 'for all': 0.06306306306306306, 'may wander': 0.02702702702702703, 'now a': 0.02702702702702703, 'my own': 0.02702702702702703, 'at any': 0.02702702702702703, 'mr holmes': 0.02702702702702703, 'ay whats': 0.02702702702702703, 'my friend': 0.02702702702702703, 'fairly by': 0.02702702702702703, 'is not': 0.02702702702702703, 'was not': 0.02702702702702703, 'was afoot': 0.036036036036036036, 'for the': 0.036036036036036036, 'your letter': 0.02702702702702703, 'i am': 0.02702702702702703, 'now count': 0.02702702702702703}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_story(markov_model, limit=100, start='my god'):\n",
        "    n = 0\n",
        "    curr_state = start\n",
        "    next_state = None\n",
        "    story = \"\"\n",
        "    story+=curr_state+\" \"\n",
        "    while n<limit:\n",
        "        next_state = random.choices(list(markov_model[curr_state].keys()),\n",
        "                                    list(markov_model[curr_state].values()))\n",
        "\n",
        "        curr_state = next_state[0]\n",
        "        story+=curr_state+\" \"\n",
        "        n+=1\n",
        "    return story\n"
      ],
      "metadata": {
        "id": "LYabmtWO_h2y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    print(str(i)+\". \", generate_story(markov_model, start=\"dear holmes\", limit=8))"
      ],
      "metadata": {
        "id": "DI6SuRI6_tL2",
        "outputId": "cb744814-e95f-485f-ec5f-18eb8c7da7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.  dear holmes i ejaculated well really he cried holmes smiled at the upper end until they came in \n",
            "1.  dear holmes what do you mean the little hampshire station we secured a ramshackle trap and in a \n",
            "2.  dear holmes i thought that i am arrested it may have nothing to do with dogs when i \n",
            "3.  dear holmes if i can not help thinking that your hat mr baker yes sir that sir charles \n",
            "4.  dear holmes i fear that you step right out of there with his blood upon my mind and \n",
            "5.  dear holmes what do you propose to investigate that let me tell me everything then said he by \n",
            "6.  dear holmes i exclaimed and then he cried with a most amazing power of sustained vindictiveness which he \n",
            "7.  dear holmes i exclaimed oh the mystery he answered coming back with a better time my dear watson \n",
            "8.  dear holmes he has not climbed nothing would annoy brother bartholomew more than any of those letters the \n",
            "9.  dear holmes i fear that it was a scourge which inflicted the injuries his circle of light thrown \n",
            "10.  dear holmes that i should dwell where none whom i served at the fatal blow and bound hand \n",
            "11.  dear holmes i fear that the barrister who was entrusted with the gentleman who lost his grip but \n",
            "12.  dear holmes if i am in positive danger overhung our father he rushed forward to protect and help \n",
            "13.  dear holmes i ejaculated surely said he with his head again i observed that he has occasional glimmerings \n",
            "14.  dear holmes you are not wounded holmes i observed that the bust asked holmes with the previous consultations \n",
            "15.  dear holmes am i to do so he had no very heavy sleeper and it has twice been \n",
            "16.  dear holmes my previous letters and papers upon the norwood police station and it was proved that she \n",
            "17.  dear holmes i thought it best to set me thinking that it was somewhere indoors so he worked \n",
            "18.  dear holmes my previous letters and telegrams which they were all gone to bed when the stranger came \n",
            "19.  dear holmes i exclaimed how on earth i cut myself off from it she would never have allowed \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}